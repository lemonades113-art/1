# -*- coding: utf-8 -*-
"""
GRPO Math ä¸»ç¨‹åº (VERLé£æ ¼é‡æ„)
================================

Usage:
    # å•å¡æ¼”ç¤º
    python main.py --mode demo
    
    # 4å¡è®­ç»ƒ
    accelerate launch --config_file accelerate_config.yaml main.py --mode train
    
    # è¯„ä¼°
    python main.py --mode eval --model_path ./outputs/grpo_math_verl/final

é¢è¯•è¯æœ¯ï¼š
"ä½¿ç”¨accelerate + DeepSpeed ZeRO-2å®ç°4å¡åˆ†å¸ƒå¼è®­ç»ƒï¼Œ
æ ¸å¿ƒç®—æ³•é‡‡ç”¨VERLçš„ç»„å†…æ ‡å‡†åŒ–Advantageè®¡ç®—ã€‚
æ”¯æŒGRPO/GSPO/RLOOä¸‰ç§ç®—æ³•åˆ‡æ¢ï¼Œ7Bæ¨¡å‹4å¡å¯è®­ç»ƒã€‚"
"""

import argparse
import json
from pathlib import Path
from typing import List, Dict

from config import CONFIG, get_config
from data_module import MathDataset, MathProblem
from reward_function import MathRewardFunction, RewardTracker
from grpo_trainer import VerlGRPOTrainer


def setup_environment():
    """ç¯å¢ƒå‡†å¤‡"""
    dirs = ["./data", "./cache", "./outputs", "./logs"]
    for d in dirs:
        Path(d).mkdir(parents=True, exist_ok=True)
    print("âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ")


def load_data(
    sources: List[str] = None,
    max_train: int = None,
    max_eval: int = None
) -> MathDataset:
    """åŠ è½½æ•°æ®"""
    dataset = MathDataset(
        sources=sources or ["gsm8k"],
        max_train=max_train or CONFIG.data.max_train_samples,
        max_eval=max_eval or CONFIG.data.max_eval_samples
    )
    dataset.load()
    return dataset


def run_demo():
    """å¿«é€Ÿæ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("ğŸ¯ GRPO Math æ¼”ç¤º (VERLé£æ ¼)")
    print("=" * 60)
    
    # åŠ è½½å°‘é‡æ•°æ®
    dataset = load_data(sources=["gsm8k"], max_train=50, max_eval=10)
    
    # æµ‹è¯•VERLç®—æ³•
    print("\nğŸ” æµ‹è¯•VERLæ ¸å¿ƒç®—æ³•:")
    from verl_algorithms import compute_grpo_outcome_advantage, compute_policy_loss_dual_clip
    import torch
    import numpy as np
    
    # æ¨¡æ‹Ÿæ•°æ®
    rewards = torch.tensor([[0.8], [0.5], [0.3], [0.9]])  # 4ä¸ªæ ·æœ¬
    mask = torch.ones(4, 1)
    index = np.array([0, 0, 1, 1])  # 2ä¸ªpromptï¼Œæ¯ä¸ª2ä¸ªå“åº”
    
    adv, _ = compute_grpo_outcome_advantage(rewards, mask, index)
    print(f"  åŸå§‹rewards: {rewards[:, 0].tolist()}")
    print(f"  GRPO advantages: {adv[:, 0].tolist()}")
    
    # æµ‹è¯•Rewardå‡½æ•°
    print("\nğŸ” æµ‹è¯•Rewardå‡½æ•°:")
    reward_fn = MathRewardFunction()
    test_cases = [
        ("Let me solve step by step. 2+2=4. #### 4", "4"),
        ("#### 4", "4"),
        ("#### 5", "4"),
    ]
    for response, gold in test_cases:
        result = reward_fn.compute(response, gold)
        print(f"  '{response[:30]}...' â†’ reward={result.total:.3f}, correct={result.is_correct}")
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼å®Œæ•´è®­ç»ƒè¯·ä½¿ç”¨:")
    print("   accelerate launch --config_file accelerate_config.yaml main.py --mode train")


def run_training(algorithm: str = "grpo", sources: List[str] = None):
    """è¿è¡Œè®­ç»ƒ"""
    print("\n" + "=" * 60)
    print(f"ğŸš€ {algorithm.upper()} Math è®­ç»ƒ (VERLé£æ ¼, åˆ†å¸ƒå¼)")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    dataset = load_data(sources=sources or ["gsm8k"])
    train_data = dataset.get_train_dataset()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = VerlGRPOTrainer(algorithm=algorithm)
    
    # åˆå§‹åŒ–
    if not trainer.setup():
        print("âŒ åˆå§‹åŒ–å¤±è´¥")
        return
    
    # è®­ç»ƒ
    trainer.train(
        train_data=train_data,
        num_epochs=CONFIG.grpo.num_epochs,
        batch_size=CONFIG.grpo.batch_size,
    )
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
    print(f"   æ¨¡å‹ä¿å­˜åˆ°: {trainer.output_dir}")


def run_evaluation(model_path: str = None):
    """è¿è¡Œè¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ğŸ“Š GRPO Math è¯„ä¼°")
    print("=" * 60)
    
    dataset = load_data(sources=["gsm8k"], max_eval=100)
    
    if model_path and Path(model_path).exists():
        print(f"   ä½¿ç”¨æ¨¡å‹: {model_path}")
        # TODO: åŠ è½½æ¨¡å‹è¿›è¡Œè¯„ä¼°
    else:
        print("   ä½¿ç”¨Rewardå‡½æ•°è¯„ä¼° (æ— æ¨¡å‹)")
    
    # ç®€å•ç»Ÿè®¡
    print(f"\n   è¯„ä¼°é›†å¤§å°: {len(dataset.eval_data)}")
    print("   (å®Œæ•´è¯„ä¼°éœ€è¦åŠ è½½è®­ç»ƒåçš„æ¨¡å‹)")


def main():
    parser = argparse.ArgumentParser(description="GRPO Math Training (VERL-style)")
    parser.add_argument(
        "--mode",
        choices=["demo", "eval", "train"],
        default="demo",
        help="è¿è¡Œæ¨¡å¼"
    )
    parser.add_argument(
        "--model_path",
        type=str,
        default=None,
        help="è¯„ä¼°æ—¶ä½¿ç”¨çš„æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--sources",
        type=str,
        default="gsm8k",
        help="æ•°æ®æºï¼ˆgsm8k,openr1ï¼‰"
    )
    parser.add_argument(
        "--algorithm",
        type=str,
        choices=["grpo", "gspo", "rloo"],
        default="grpo",
        help="è®­ç»ƒç®—æ³•"
    )
    
    args = parser.parse_args()
    
    setup_environment()
    sources = args.sources.split(",")
    
    if args.mode == "demo":
        run_demo()
    elif args.mode == "eval":
        run_evaluation(args.model_path)
    elif args.mode == "train":
        run_training(algorithm=args.algorithm, sources=sources)


if __name__ == "__main__":
    main()
